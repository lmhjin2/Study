# 28 ~ 55

# 이 코드는 COCO 2017 데이터셋을 사용하여 이미지와 해당 설명(caption)을 처리하고 준비하는 과정을 보여줌
# Embedding이란? 

# 59 ~ 72

# 이 코드 부분은 TensorFlow를 사용하여 자연어 처리 작업에 필요한 토큰화 과정을 설정하고 실행하는 데 관련된 설정과 단계들을 설명

# TextVectorization : 텍스트 데이터를 수치형 벡터 데이터로 변환하는 과정을 자동화, 초기 데이터 전처리 단계에서 필요한 텍스트 처리를 효율적으로 수행
# 여기서 말한 어휘는 모델이 이해할수있는 숫자로 매핑한다는 의미이다

    
# 78 ~ 89

# 이 코드는 토큰화된 텍스트 데이터를 처리하고, 단어와 정수 인덱스 간의 매핑을 생성 및 저장하는 과정을 구현합니다.
# pickle을 사용하여 어휘 목록을 파일에 저장하고, StringLookup 층을 사용하여 단어-인덱스 및 인덱스-단어 매핑을 생성합니다.
# 이러한 매핑은 텍스트 데이터를 모델이 처리할 수 있는 형태로 변환하고, 모델의 출력을 사람이 이해할 수 있는 텍스트로 다시 변환하는 데 필수적입니다.

# mask_token : StringLookup 레이어에서 텍스트 이미지를 수치 데이터로 변환시켜 주는데 이 과정에서 mask_token 매개변수에 지정된 친구들은 변환과정에서 무시된다.
#              즉, 우리가 설정한 "" 은 빈문자열을 마스킹 하겠다는 것인데 이게 곧 전처리 과정에서 빈문자열이 들어오면 무시하고 처리할때 제외시켜 달라는 뜻이다.


# 93 ~ 103
# 이 코드는 이미지 캡션 데이터셋을 처리하여, 각 이미지에 대한 캡션들을 그룹화하고, 이후에 이 데이터를 훈련 세트와 검증 세트로 분할하는 과정을 보여줌
# collections 모듈을 임포트합니다. 이 모듈은 컨테이너 데이터 타입을 제공
# 컨테이너 데이터 타입 : 여러 요소를 담을 수 있는 데이터 구조 (리스트, 튜플, 딕셔너리)

# 104 ~ 109
# 데이터셋의 모든 이미지와 캡션을 무작위로 섞은 후, 80%는 훈련 데이터로, 나머지 20%는 검증 데이터로 분할하는 작업을 수행


# 108 ~ 121








# 162~ 177
# AUTOTUNE : 병렬처리
# 메모리 사용 최적화, 속도 향상: 데이터셋의 각 요소를 동시에 처리함으로써 데이터 로딩 및 전처리에 걸리는 시간을 대폭 줄일 수 있고 이는 특히 큰 이미지 데이터셋을 다룰 때 중요하며, 모델 학습 시간을 단축
# 학습 과정의 안정성


# TransformerEncoderLayer
# TransformerEncoderLayer 클래스는 Transformer 모델의 인코더 층을 구성하는 주요 요소를 포함합니다. 
# 자가 주의 메커니즘을 통해 입력 데이터의 다양한 부분 간의 관계를 학습하고, 
# 층 정규화와 포지션 와이즈 피드포워드 신경망을 통해 모델의 표현력을 향상시킵니다. 
# 이러한 구조는 Transformer 모델이 복잡한 시퀀스 데이터를 효과적으로 처리할 수 있게 해줍니다.

#Transformer 모델 구조에서 "포지션 와이즈(Position-wise) 피드포워드 신경망(Feedforward Neural Network, FFNN)"이란?
# 모델의 각 위치(즉, 시퀀스 내 각 단어나 토큰의 위치)에 동일한 신경망을 적용, 
# 이 신경망은 주로 두 개의 밀집(Dense) 층과 활성화 함수로 구성되며, 입력 시퀀스의 각 위치에서 동일한 연산을 수행하여 비선형 변환을 제공

# 포지션 와이즈 피드포워드 신경망의 역할
# Transformer 모델의 각 인코더와 디코더 층에는 멀티 헤드 어텐션 메커니즘 다음에 포지션 와이즈 피드포워드 신경망이 위치함.
# 이 신경망의 주요 목적은 어텐션 메커니즘으로부터 나온 출력에 추가적인 비선형 변환을 적용하여, 모델이 더 복잡한 패턴을 학습할 수 있도록 하는 것


# Attention Score
# 어텐션 스코어는 입력 시퀀스 내의 각 단어(또는 토큰)가 현재 처리 중인 단어(또는 토큰)에 대해 얼마나 중요한지를 나타내는 수치입니다. 
# 이 스코어는 일반적으로 쿼리(Query), 키(Key), 밸류(Value)라는 세 가지 요소를 사용하여 계산됩니다. 
# 쿼리와 각 키 사이의 유사도(예: 내적)를 계산하여 어텐션 스코어를 얻고, 이 스코어를 사용하여 밸류들의 가중합을 계산하여 최종 출력을 생성
# Dropout
# 계산된 어텐션 스코어 중 일부를 임의로 0으로 만들어, 모델이 어텐션 스코어의 작은 변화에도 견고하게 반응하도록 만든다.
# 모델의 일반화 능력을 향상시키는 데 도움이 됩니다

# 어텐션 스코어는 모델이 입력 데이터의 어떤 부분에 주목해야 하는지 결정하는 데 사용되는 중요도 지표다
# 드롭아웃 비율은 이러한 어텐션 메커니즘의 과적합을 방지하고 모델의 견고성을 높이기 위해 사용되는 기법 중 하나임.

# image captioning model
# accuracy 계산 정의 함수

# tf.argmax(y_pred, axis=n)의 axis 매개변수에 따른 작동 방식은 다음과 같습니다:

# Axis=0: 이 경우 함수는 각 특징(또는 시간 스텝, 열 등)과 클래스에 대해 전체 배치에 걸쳐 최대 값의 인덱스를 찾습니다.
# 예를 들어, 배치에 여러 개의 예측이 있을 때 각 특징별로 가장 높은 값을 가진 배치 항목의 인덱스를 찾습니다.

# Axis=1: 이 경우 함수는 배치의 각 항목에 대해 각 시간 스텝(또는 행)에서 최대 값의 인덱스를 찾습니다. 
# 시퀀스 또는 시간에 따른 변화를 분석할 때 유용합니다. 예를 들어, 시간 스텝 별로 가장 가능성 높은 클래스의 인덱스를 찾습니다.

# Axis=2: 이 경우 함수는 배치의 각 항목과 각 시간 스텝에 대해 각 특징(또는 클래스)에서 최대 값의 인덱스를 찾습니다. 
# 이는 다중 클래스 분류 문제에서 각 예측에 대해 가장 확률이 높은 클래스의 인덱스를 찾는 데 사용됩니다.














