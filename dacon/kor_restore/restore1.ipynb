{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 단어 사전 기반 매핑 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv', encoding = 'utf-8-sig')\n",
    "test = pd.read_csv('./test.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 단어 사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dict = {}\n",
    "\n",
    "for input_text, output_text in zip(train['input'], train['output']):\n",
    "    input_words = input_text.split()\n",
    "    output_words = output_text.split()\n",
    "    for iw, ow in zip(input_words, output_words):\n",
    "        match_dict[iw] = ow  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 변환 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words(input_text, match_dict):\n",
    "    words = input_text.split() \n",
    "    replaced_words = [match_dict.get(word, word) for word in words] \n",
    "    return \" \".join(replaced_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_reviews = test['input'].apply(lambda x: replace_words(x, match_dict)).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['output'] = converted_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./baseline_submission.csv', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LLM 활용 (Gemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv', encoding = 'utf-8-sig')\n",
    "test = pd.read_csv('./test.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "for i in range(10):\n",
    "    sample = f\"input : {train['input'][i]} \\n output : {train['output'][i]}\"\n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type= 'nf4',\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model_id = 'beomi/gemma-ko-7b'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config = bnb_config, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer \n",
    ")\n",
    "\n",
    "restored_reviews = []\n",
    "\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    query = row['input']  \n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful assistant specializing in restoring obfuscated Korean reviews. \"\n",
    "                \"Your task is to transform the given obfuscated Korean review into a clear, correct, \"\n",
    "                \"and natural-sounding Korean review that reflects its original meaning. \"\n",
    "                \"Below are examples of obfuscated Korean reviews and their restored forms:\\n\\n\"\n",
    "                f\"Example, {samples}\"  \n",
    "                \"Spacing and word length in the output must be restored to the same as in the input. \"\n",
    "                \"Do not provide any description. Print only in Korean.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"input : {query}, output : \"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    prompt = \"\\n\".join([m[\"content\"] for m in messages]).strip()\n",
    "    \n",
    "\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=len(query),\n",
    "        eos_token_id=pipe.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = outputs[0]['generated_text']\n",
    "    result = generated_text[len(prompt):].strip()\n",
    "        \n",
    "\n",
    "    restored_reviews.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['output'] = restored_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./baseline_submission.csv', index = False, encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
